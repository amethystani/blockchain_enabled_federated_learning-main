"""
Automated Threshold Tuning via Cross-Validation

Automatically calibrates detection thresholds using cross-validation on
honest client data to minimize false positives while maximizing detection.

From WHATWEHAVETOIMPLEMENT.MD Line 15: "automated threshold tuning with 
cross-validation under Non-IID data"
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt


class AutomatedThresholdTuner:
    """
    Automatic threshold calibration via cross-validation.
    
    Uses honest client gradients to find optimal detection threshold
    that balances false positives vs. detection rate.
    """
    
    def __init__(
        self,
        n_folds: int = 5,
        target_fpr: float = 0.05,  # Target false positive rate
        search_resolution: int = 100
    ):
        """
        Initialize tuner.
        
        Args:
            n_folds: Number of cross-validation folds
            target_fpr: Target false positive rate
            search_resolution: Number of threshold candidates
        """
        self.n_folds = n_folds
        self.target_fpr = target_fpr
        self.search_resolution = search_resolution
        self.best_threshold = None
        self.calibration_curve = None
    
    def tune_threshold(
        self,
        honest_gradients: List[Dict[str, torch.Tensor]],
        method: str = 'spectral_norm'
    ) -> float:
        """
        Tune detection threshold using cross-validation.
        
        Args:
            honest_gradients: List of honest client gradients
            method: Detection method ('spectral_norm', 'max_eigenvalue')
            
        Returns:
            Optimal threshold
        """
        print(f"\nðŸŽ¯ Tuning threshold with {self.n_folds}-fold CV...")
        print(f"Target FPR: {self.target_fpr:.1%}")
        print(f"Method: {method}")
        
        # Compute detection statistics for all gradients
        statistics = [self._compute_statistic(g, method) for g in honest_gradients]
        statistics = np.array(statistics)
        
        # Define threshold candidates
        min_stat, max_stat = statistics.min(), statistics.max()
        thresholds = np.linspace(min_stat, max_stat, self.search_resolution)
        
        # Cross-validation
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
        fpr_per_threshold = []
        
        for threshold in thresholds:
            fprs = []
            for train_idx, val_idx in kfold.split(statistics):
                val_stats = statistics[val_idx]
                # False positive rate: fraction of honest flagged as Byzantine
                fpr = (val_stats > threshold).mean()
                fprs.append(fpr)
            
            fpr_per_threshold.append(np.mean(fprs))
        
        fpr_per_threshold = np.array(fpr_per_threshold)
        
        # Find threshold closest to target FPR
        idx = np.argmin(np.abs(fpr_per_threshold - self.target_fpr))
        best_threshold = thresholds[idx]
        actual_fpr = fpr_per_threshold[idx]
        
        self.best_threshold = best_threshold
        self.calibration_curve = (thresholds, fpr_per_threshold)
        
        print(f"\nâœ“ Optimal threshold: {best_threshold:.6f}")
        print(f"  Achieved FPR: {actual_fpr:.1%}")
        print(f"  Target FPR: {self.target_fpr:.1%}")
        
        return best_threshold
    
    def _compute_statistic(
        self,
        gradient: Dict[str, torch.Tensor],
        method: str
    ) -> float:
        """Compute detection statistic."""
        
        # Flatten gradient
        flat = torch.cat([v.flatten() for v in gradient.values()])
        
        if method == 'spectral_norm':
            # L2 norm
            return torch.norm(flat).item()
        
        elif method == 'max_eigenvalue':
            # Approximate largest eigenvalue via power iteration
            # (simplified - full implementation would use covariance matrix)
            return torch.norm(flat).item()
        
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def plot_calibration_curve(self, save_path: Optional[str] = None):
        """Plot threshold vs FPR calibration curve."""
        
        if self.calibration_curve is None:
            raise ValueError("Must call tune_threshold first")
        
        thresholds, fprs = self.calibration_curve
        
        plt.figure(figsize=(10, 6))
        plt.plot(thresholds, fprs, 'b-', linewidth=2, label='FPR')
        plt.axhline(y=self.target_fpr, color='r', linestyle='--', 
                   label=f'Target FPR ({self.target_fpr:.1%})')
        plt.axvline(x=self.best_threshold, color='g', linestyle='--',
                   label=f'Optimal Threshold ({self.best_threshold:.4f})')
        
        plt.xlabel('Detection Threshold', fontsize=12)
        plt.ylabel('False Positive Rate', fontsize=12)
        plt.title('Threshold Calibration Curve', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“Š Calibration curve saved: {save_path}")
        else:
            plt.show()
    
    def validate_threshold(
        self,
        honest_gradients: List[Dict[str, torch.Tensor]],
        byzantine_gradients: List[Dict[str, torch.Tensor]],
        method: str = 'spectral_norm'
    ) -> Dict[str, float]:
        """
        Validate tuned threshold on held-out data.
        
        Args:
            honest_gradients: Honest gradients for FPR
            byzantine_gradients: Byzantine gradients for TPR
            method: Detection method
            
        Returns:
            Dict with FPR, TPR, F1
        """
        if self.best_threshold is None:
            raise ValueError("Must call tune_threshold first")
        
        # Compute statistics
        honest_stats = [self._compute_statistic(g, method) for g in honest_gradients]
        byzantine_stats = [self._compute_statistic(g, method) for g in byzantine_gradients]
        
        # Detection
        honest_flagged = np.array([s > self.best_threshold for s in honest_stats])
        byzantine_flagged = np.array([s > self.best_threshold for s in byzantine_stats])
        
        fpr = honest_flagged.mean()
        tpr = byzantine_flagged.mean()
        
        # F1 score
        if tpr + fpr > 0:
            precision = tpr / (tpr + fpr)
            f1 = 2 * (precision * tpr) / (precision + tpr) if precision + tpr > 0 else 0
        else:
            f1 = 0
        
        results = {
            'fpr': fpr,
            'tpr': tpr,
            'f1': f1,
            'threshold': self.best_threshold
        }
        
        print(f"\nðŸ“Š Validation Results:")
        print(f"  FPR: {fpr:.1%}")
        print(f"  TPR (Detection Rate): {tpr:.1%}")
        print(f"  F1 Score: {f1:.3f}")
        
        return results


class AdaptiveThresholdTracker:
    """
    Online threshold adaptation using sliding window.
    
    Updates threshold dynamically as training progresses and data
    distribution shifts.
    """
    
    def __init__(self, window_size: int = 50, update_frequency: int = 5):
        """
        Initialize adaptive tracker.
        
        Args:
            window_size: Number of rounds to include in sliding window
            update_frequency: Update threshold every N rounds
        """
        self.window_size = window_size
        self.update_frequency = update_frequency
        self.statistics_history = []
        self.threshold_history = []
        self.round_count = 0
    
    def update(
        self,
        statistics: List[float],
        target_fpr: float = 0.05
    ) -> float:
        """
        Update threshold based on recent statistics.
        
        Args:
            statistics: Detection statistics from current round
            target_fpr: Target false positive rate
            
        Returns:
            Updated threshold
        """
        self.round_count += 1
        self.statistics_history.extend(statistics)
        
        # Keep only recent history
        if len(self.statistics_history) > self.window_size * 10:
            self.statistics_history = self.statistics_history[-self.window_size * 10:]
        
        # Update threshold periodically
        if self.round_count % self.update_frequency == 0:
            recent = np.array(self.statistics_history[-self.window_size:])
            # Threshold = percentile corresponding to (1 - target_fpr)
            threshold = np.percentile(recent, (1 - target_fpr) * 100)
            self.threshold_history.append((self.round_count, threshold))
            return threshold
        
        # Return previous threshold
        if self.threshold_history:
            return self.threshold_history[-1][1]
        
        # Initial threshold
        return np.percentile(statistics, 95)


if __name__ == '__main__':
    # Example usage
    print("Automated Threshold Tuning Demo")
    
    # Simulate honest gradients
    np.random.seed(42)
    honest = []
    for _ in range(50):
        grad = {'param': torch.randn(100)}
        honest.append(grad)
    
    # Tune threshold
    tuner = AutomatedThresholdTuner(n_folds=5, target_fpr=0.05)
    threshold = tuner.tune_threshold(honest, method='spectral_norm')
    
    # Plot calibration
    tuner.plot_calibration_curve('threshold_calibration.png')
    
    print("\nâœ“ Automated threshold tuning complete!")
